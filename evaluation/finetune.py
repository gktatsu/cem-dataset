import os, sys, argparse, mlflow, yaml
import numpy as np
import torch
import torch.nn as nn
import segmentation_models_pytorch as smp
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader

from albumentations import (
    Compose, PadIfNeeded, Normalize, HorizontalFlip, VerticalFlip, RandomBrightnessContrast, 
    CropNonEmptyMaskIfExists, GaussNoise, RandomResizedCrop, Rotate, GaussianBlur
)
from albumentations.pytorch import ToTensorV2

from data import SegmentationData, FactorResize
from train_utils import Trainer
from utils import load_pretrained_state_for_unet, moco_to_unet_prefixes

augmentation_dict = {
    'PadIfNeeded': PadIfNeeded, 'HorizontalFlip': HorizontalFlip, 'VerticalFlip': VerticalFlip,
    'RandomBrightnessContrast': RandomBrightnessContrast, 'CropNonEmptyMaskIfExists': CropNonEmptyMaskIfExists,
    'GaussNoise': GaussNoise, 'RandomResizedCrop': RandomResizedCrop, 'Rotate': Rotate, 
    'GaussianBlur': GaussianBlur
}

def parse_args():
    #setup the argument parser
    parser = argparse.ArgumentParser(description='Runs finetuning on 2d segmentation data')
    
    #get the config file
    parser.add_argument('config', type=str, metavar='pretraining', help='Path to a config yaml file')

    #return the arguments converted to a dictionary
    return vars(parser.parse_args())

if __name__ == "__main__":
    if 'snakemake' in globals():
        args = vars(snakemake.params)
        del args['_names']
    else:
        args = parse_args()
        
    #set manual seed to ensure we always start with the same model parameters
    torch.manual_seed(42)
    
    #load the config file
    with open(args['config'], 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
        
    #add the path to the config file for archiving
    config['config_file'] = args['config']

    #extract the experiment name
    experiment = config['experiment_name']

    #set the transforms to use, we're using imagenet
    #pretrained models, so we want to use the default
    #normalization parameters
    pretraining = config['pretraining']
    
    #if we're working with MoCo pretrained weights
    #then we'll have to download them separately from the 
    #built-in pytorch function
    if pretraining in ['imagenet_mocov2', 'cellemnet_mocov2']:
        state_dict, norms = load_pretrained_state_for_unet(config['encoder'], pretraining)
        if norms == None:
            gray_channels = 3
            normalize = Normalize() #default is ImageNet means and standard deviations
        else:
            gray_channels = 1
            normalize = Normalize(mean=norms[0], std=norms[1])
        
        #create the Unet model and load the pretrained weights
        model = smp.Unet(config['encoder'], in_channels=gray_channels, encoder_weights=None, classes=config['num_classes'])
        msg = model.load_state_dict(state_dict, strict=False)
    elif pretraining == 'imagenet_supervised':
        model = smp.Unet(config['encoder'], encoder_weights='imagenet', classes=config['num_classes'])
        gray_channels = 3
        normalize = Normalize() #default is ImageNet means and standard deviations
    elif os.path.isfile(pretraining):
        #it's also possible to directly pass a .pth file as the
        #pretrained weights. In which case we assume that they
        #were generated by the MoCo script and load them accordingly
        checkpoint = torch.load(pretraining, map_location='cpu')
        state_dict, norms = checkpoint['state_dict'], checkpoint['norms']
        state_dict = moco_to_unet_prefixes(state_dict)
        gray_channels = 1
        normalize = Normalize(mean=norms[0], std=norms[1])
        
        #create the Unet model and load the pretrained weights
        model = smp.Unet(config['encoder'], in_channels=gray_channels, encoder_weights=None, classes=config['num_classes'])
        msg = model.load_state_dict(state_dict, strict=False)
    else: #random initialization
        print('No valid pretraining found. Using randomly initialized weights!')
        gray_channels = 1
        model = smp.Unet(config['encoder'], in_channels=gray_channels, encoder_weights=None, classes=config['num_classes'])
        normalize = Normalize(**config['norms']) #use the norms defined for the dataset in the config file

    #freeze all encoder layers to start and only open
    #them when specified
    for param in model.encoder.parameters():
        param.requires_grad = False

    #unfreeze layers based on the finetune_layer argument
    finetune_layer = config['finetune_layer']
    encoder_groups = [mod[1] for mod in model.encoder.named_children()]
    if finetune_layer != 'none':
        layer_index = {'all': 0, 'layer1': 4, 'layer2': 5, 'layer3': 6, 'layer4': 7}
        start_layer = layer_index[finetune_layer]

        #always finetune from the start layer to the last layer in the resnet
        for group in encoder_groups[start_layer:]:
            for param in group.parameters():
                param.requires_grad = True
                
    if config['unfreeze_encoder_bn']:
        def unfreeze_encoder_bn(module):
            if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):
                for param in module.parameters():
                    param.requires_grad = True

        model.encoder.apply(unfreeze_encoder_bn)
                
    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    params = sum([np.prod(p.size()) for p in model_parameters])
    print(f'Using model with {params} trainable parameters!')
    
    #construct set of augmentations from config
    dataset_augs = []
    for aug_params in config['augmentations']:
        aug_name = aug_params['aug']
        
        #lookup this name and replace it with the 
        #correct augmentation class
        aug = augmentation_dict[aug_name]
        
        #delete the aug key and then the remaining
        #dictionary items are kwargs
        del aug_params['aug']
        dataset_augs.append(aug(**aug_params))
        
    #unpack the list of dataset specific augmentations
    #into Compose, and then add normalization and tensor
    #conversion, which apply universally
    augs = Compose([
        *dataset_augs,
        normalize,
        ToTensorV2()
    ])
    
    #create pytorch datasets
    data_dir = config['data_dir']
    train_dir = 'train/'
        
    bsz = config['bsz']
    trn_data = SegmentationData(os.path.join(data_dir, train_dir), tfs=augs, gray_channels=gray_channels, 
                        segmentation_classes=config['num_classes'])
    config['n_images'] = len(trn_data.fnames)
    
    train = DataLoader(trn_data, batch_size=bsz, shuffle=True, pin_memory=True, drop_last=True, num_workers=config['jobs'])
     
    #check for a validation directory and use it if it exists
    #if not, then we don't use any validation data
    val_dir = 'valid/'
    if os.path.isdir(os.path.join(data_dir, val_dir)):
        #eval_augs are always the same
        eval_augs = Compose([
            FactorResize(32),
            normalize,
            ToTensorV2()
        ])
            
        val_data = SegmentationData(os.path.join(data_dir, val_dir), tfs=eval_augs, gray_channels=gray_channels, 
                                    segmentation_classes=config['num_classes'])
        valid = DataLoader(val_data, batch_size=1, shuffle=False, pin_memory=True, num_workers=config['jobs'])
    else:
        valid = None
    
    #create model path ahead of time
    model_dir = config['model_dir']
    if not os.path.isdir(model_dir):
        os.mkdir(model_dir)
        
    cudnn.benchmark = True
    trainer = Trainer(config, model, train, valid)
    trainer.train()