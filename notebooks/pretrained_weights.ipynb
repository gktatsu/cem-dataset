{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained weight usage\n",
    "\n",
    "A minimum example of how to load and use the CEM500K pre-trained weights for classification or segmentation tasks.\n",
    "\n",
    "Before getting started download CEM500K data and models from EMPIAR:\n",
    "- EMPIAR entry: https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/\n",
    "- Download help: https://www.ebi.ac.uk/pdbe/emdb/empiar/faq#question_Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider a simple binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from copy import deepcopy\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_empiar_download = '' #fill this in\n",
    "state_path = os.path.join(path_to_empiar_download, 'pretrained_models/cem500k_mocov2_resnet50_200ep_pth.tar')\n",
    "state = torch.load(state_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epoch', 'arch', 'state_dict', 'optimizer', 'norms']\n"
     ]
    }
   ],
   "source": [
    "#take a look at what's inside the state\n",
    "print(list(state.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epoch: the training epoch when state was recorded\n",
    "- Arch: the model architecture: \"resnet50\"\n",
    "- State_dict: state dict for the complete pretrained model (both query and key encoders)\n",
    "- Optimizer: state of the optimizer at save (useful for resuming training)\n",
    "- Norms: the mean and std pixel values used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the parameter names to match torchvision resnet50\n",
    "resnet50_state_dict = deepcopy(state_dict)\n",
    "for k in list(resnet50_state_dict.keys()):\n",
    "    #only keep query encoder parameters; discard the fc projection head\n",
    "    if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
    "        resnet50_state_dict[k[len(\"module.encoder_q.\"):]] = resnet50_state_dict[k]\n",
    "\n",
    "    #delete renamed or unused k\n",
    "    del resnet50_state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create model and load the pretrained weights\n",
    "model = resnet50()\n",
    "\n",
    "#overwrite the first conv layer to accept single channel grayscale image\n",
    "#overwrite the fc layer for binary classification\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(2048, 1, bias=True)\n",
    "\n",
    "#loads all parameters but those for the fc head\n",
    "#those parameters need to be trained\n",
    "model.load_state_dict(resnet50_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the parameters into a simple binary segmentation UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as before we need to update parameter names to match the UNet model\n",
    "#for segmentation_models_pytorch we simply and the prefix \"encoder.\"\n",
    "#format the parameter names to match torchvision resnet50\n",
    "unet_state_dict = deepcopy(resnet50_state_dict)\n",
    "for k in list(unet_state_dict.keys()):\n",
    "    unet_state_dict['encoder.' + k] = unet_state_dict[k]\n",
    "    del unet_state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['decoder.blocks.0.conv1.0.weight', 'decoder.blocks.0.conv1.1.weight', 'decoder.blocks.0.conv1.1.bias', 'decoder.blocks.0.conv1.1.running_mean', 'decoder.blocks.0.conv1.1.running_var', 'decoder.blocks.0.conv2.0.weight', 'decoder.blocks.0.conv2.1.weight', 'decoder.blocks.0.conv2.1.bias', 'decoder.blocks.0.conv2.1.running_mean', 'decoder.blocks.0.conv2.1.running_var', 'decoder.blocks.1.conv1.0.weight', 'decoder.blocks.1.conv1.1.weight', 'decoder.blocks.1.conv1.1.bias', 'decoder.blocks.1.conv1.1.running_mean', 'decoder.blocks.1.conv1.1.running_var', 'decoder.blocks.1.conv2.0.weight', 'decoder.blocks.1.conv2.1.weight', 'decoder.blocks.1.conv2.1.bias', 'decoder.blocks.1.conv2.1.running_mean', 'decoder.blocks.1.conv2.1.running_var', 'decoder.blocks.2.conv1.0.weight', 'decoder.blocks.2.conv1.1.weight', 'decoder.blocks.2.conv1.1.bias', 'decoder.blocks.2.conv1.1.running_mean', 'decoder.blocks.2.conv1.1.running_var', 'decoder.blocks.2.conv2.0.weight', 'decoder.blocks.2.conv2.1.weight', 'decoder.blocks.2.conv2.1.bias', 'decoder.blocks.2.conv2.1.running_mean', 'decoder.blocks.2.conv2.1.running_var', 'decoder.blocks.3.conv1.0.weight', 'decoder.blocks.3.conv1.1.weight', 'decoder.blocks.3.conv1.1.bias', 'decoder.blocks.3.conv1.1.running_mean', 'decoder.blocks.3.conv1.1.running_var', 'decoder.blocks.3.conv2.0.weight', 'decoder.blocks.3.conv2.1.weight', 'decoder.blocks.3.conv2.1.bias', 'decoder.blocks.3.conv2.1.running_mean', 'decoder.blocks.3.conv2.1.running_var', 'decoder.blocks.4.conv1.0.weight', 'decoder.blocks.4.conv1.1.weight', 'decoder.blocks.4.conv1.1.bias', 'decoder.blocks.4.conv1.1.running_mean', 'decoder.blocks.4.conv1.1.running_var', 'decoder.blocks.4.conv2.0.weight', 'decoder.blocks.4.conv2.1.weight', 'decoder.blocks.4.conv2.1.bias', 'decoder.blocks.4.conv2.1.running_mean', 'decoder.blocks.4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.Unet('resnet50', in_channels=1, encoder_weights=None, classes=1)\n",
    "#all encoder parameters are loaded\n",
    "#parameters in the decoder must be trained on task data\n",
    "model.load_state_dict(unet_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation_models_pytorch module comes with a selection of state-of-the-art semantic segmentation models. The weight loading procedure is the same for all of these architectures. For a full list, see https://github.com/qubvel/segmentation_models.pytorch#models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
